---
title: "Cleantweets"
author: "Noblezada, Camasa, Cabia"
date: "2024-12-13"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
library(tidyverse)
library(tidytext)
library(dplyr)
library(stringr)
library(ggplot2)
library(sentimentr)
```
```{r}
# Load the dataset
tweetsDF <- read_csv("/cloud/project/ProjectDS/tweetsDF.csv")

# Clean the tweet text
tweetsDF <- tweetsDF %>%
  mutate(
    text = text %>%
      iconv(from = "UTF-8", to = "ASCII//TRANSLIT", sub = "") %>% 
      tolower() %>% 
      str_remove_all("https\\S+") %>% 
      str_remove_all("[#\\n]") %>% 
      str_remove_all("[@?]\\S+") %>% 
      str_remove_all("\\?") %>% 
      str_remove_all("\\b\\d{2}\\.\\d{2}\\.\\d{4}\\b") %>%
      str_remove_all("<a href=httptwitter.comdownloadiphone rel=nofollow>twitter for iphone<a>") %>% 
      str_remove_all("<a href=([^>]*?) rel=nofollow>([^<]*?)<a>") %>%
      str_remove_all("<a href=httptwitter.comdownloadandroid rel=nofollow>twitter for android<a>") %>%
      str_remove_all("<a href= rel=nofollow>twitter web app<a>") %>%
      str_remove_all("30102022") %>% 
      str_squish() 
  )

# Function to create chunks of data
create_chunks <- function(df, start_row, end_row) {
  return(df[start_row:end_row, ])
}

# Define chunk size
start_row <- 1
end_row <- 1000

# Extract chunk of data
chunk_data <- create_chunks(tweetsDF, start_row, end_row)

# Print cleaned dataset
print(tweetsDF)
```


